\documentclass[sigconf]{acmart}

\input{format/i523}

\begin{document}
\title{Predicting Profitable Customers in Banking Industry}


\author{Dhanya Mathew}
\orcid{HID328}
\affiliation{%
  \institution{Indiana University}
  \streetaddress{711 N Park Ave}
  \city{Bloomington} 
  \state{Indiana} 
  \postcode{47408}
}
\email{dhmathew@iu.edu}


% The default list of authors is too long for headers}
\renewcommand{\shortauthors}{G. v. Laszewski}


\begin{abstract}
Banks often want to know the profile of their profitable top 1\% or 20\% customers looks like. Conversely they may also wonder what the general profile is of the customers in the worst 1\% and 20\% of profit. Based on customerâ€™s data variables at any given time, a good predictive model can predict which profit group (extremely unprofitable, average, extremely profitable, etc.) customers falls into. This helps financial institutions to better understand what drives the customer profit and accordingly take decisions to sell their products to the right customers. Further down in banking sector, it is a challenge to identify customers who are most likely to repay a loan. Recent big data and machine learning technologies have the potential to predict good customers and open doors for banks to profitable growth. Since the banking sector has evolved over the periods, there are tremendous amount of historical data available to analyze. We show how bank's big data can be analyzed and create a model based on that, to classify customers. In addition to big data technologies, we used machine learning algorithms to build the predictive model to predict creditworthy and uncreditworthy customers from a list of new customers. Random forest classification algorithm is used to achieve this goal.

\end{abstract}

\keywords{i523, HID328, Big Data, Spark, Python, Decision Trees , Random Forest}


\maketitle



\section{Introduction}

Big data as the name implies, refers to large and complex data which continues to grow enormously day by day. Industries like financial firms, in particular, have widely adopted big data analytics to obtain better investment decisions with consistent growth. Recent survey research indicates that 71 percent of firms in the financial services industry at a global level are exploring big data and predictive analytics \cite{accenture-next-generation-financial}. This number continues to grow and sectors like government, business, technology, universities, health-care, finance, manufacturing etc make use of big data to obtain meaningful information using big data technologies \cite{wiki-bigdata}. 

The finance sector contributes to the daily data generation from products and marketing, banking, business, share market etc. Banking is a very sensitive field and any useful insight can make a positive impact on the overall turnover. Historic data analysis and real time data analysis are equally important in terms of banking sector. The era of big data helps financial firms to take quality business decisions related to expanding revenues, managing costs, hiring resources etc, based on effective data analysis which provide access to real-time insights. Data-driven decision making is one of the key advantages of big data technologies.

\subsection{Project Goals}

This project aims to help banking sector to identify profitable customers. Specifically, help banks to take a decision whether to approve or reject a loan application. When a new customer approaches the bank for a loan, banks would be able to identify the customers who are most likely to repay the loan by analyzing the applicant's profile and background information. 

There can be two scenarios of risks associated with the bank's decision. First, if the customer is creditworthy and if the bank rejects the loan, then it is a loss to the bank in terms of interest. Second, if the customer is uncreditworthy and if the bank approves the loan, then it is a loss to the bank in terms of loan amount and interest. Approving loan for an uncreditworthy customer will end up in more financial loss for the bank and accordingly is a greater risk. Hence banks would require a decision rule to follow for whom to approve the loan. With our model we are trying to mitigate these risks for the banks and contribute to the decision rules. In other words, our model helps to minimize the risks and maximize the profit.

\subsection{Methods and Technologies Involved}

The goal of most of the big data projects are to analyze the Data and derive knowledge out of it. In other words, data is the input to the model and knowledge is the output. We also follow the same methods and processes for our project as shown in Figure \ref{fig:Figure1}

\begin{figure}[htb]
  \centering
  \includegraphics[width=1.0\columnwidth]{project/images/Figure1.png}
  \caption{Project Workflow
  \cite{preprocessing}}
  \label{fig:Figure1} 
\end{figure}

We took a sample data set of loan applications from customers to a bank. In the real scenarios, we will not be able to apply analytical methods directly on the raw data as it likely be imperfect and containing irrelevant information. Hence we do data cleaning as the first step. Here we did data cleaning using spark. The cleaned data has 1000 customer records with 20 variables.

We did exploratory analysis like Chi-square test to understand data and for feature selection for analysis. We also did some graphical representations to show how the actual data is related and what are direct insights available from the cleaned data set.

To develop the model we first split the data set into two parts- training data and test data. We defined 2 baseline models, Trees and finally developed the Random Forest model. We compare all these models to identify the most effective and least penalty model. We did visualization and modelling using python.

\section{Data Set}

We used the German Credit data which is publically available in the UCI Machine Learning Repository \cite{uci} and also in the website of PennState Eberly College of Science \cite{psu-site}. Both these sites have the cleaned dataset and not the original one. Hence we recreated the original one from these data sets to understand and try out the data cleaning processes. Data sets include 1000 customer records and 20 variables and a class variable. In the class variable the actual class of the customer is specified - good or bad. The original data set in excel format is available in the github repository \cite{github}. Figure \ref{fig:Figure2} shows the first 10 rows of the original data set.

\begin{figure}[htb]
  \centering
  \includegraphics[width=1.0\columnwidth]{project/images/Figure2.png}
  \caption{First 10 rows of original data set
  \cite{github}}
  \label{fig:Figure2} 
\end{figure}

\begin{figure}[htb]
  \centering
  \includegraphics[width=1.0\columnwidth]{project/images/Figure3.png}
  \caption{First 10 rows of cleaned data set
  \cite{psu-site}}
  \label{fig:Figure3} 
\end{figure}

The cleaned data set we used in this project is taken from the website of PennState Eberly College of Science \cite{psu-site} and it is uploaded in the github repository \cite{github}. Figure \ref{fig:Figure3} shows first 10 rows from the cleaned data set. The complete list of data set variables and their description is given in Table \ref{tab:table1}. 

\begin{table}
  \caption{Variables and description
  \cite{psu-site}}
  \label{tab:table1}
  \begin{tabular}{cc}
    \toprule
     Variable& Description\\
    \midrule
    Credit& Creditability: Good or Bad\\
    Account Status& Balance of current account\\
    Credit Months& Duration of Credit (month)\\
    Credit History& Payment Status of Previous Credit\\
    Purpose& Purpose of credit\\
    Credit Amount& Amount of credit\\
    Savings& Value Savings or stocks\\
    Employment& Length of current employment\\
    Installment Rate& Installment in \% of current income\\
    Personal Status& Sex and Marital Status\\
    Guarantors& Further debtors\\
    Residence& Duration in Current address\\
    Property& Most valuable available asset\\
    Age& Age in years\\
    Other Installments& Concurrent Credits\\
    Housing& Type of apartment\\
    Credit Cards& No of Credits at this Bank\\
    Occupation& Occupation\\
    Dependents& No of dependents\\
    Telephone& Phone number\\
    Foreign Worker& Foreign worker\\
    \bottomrule
  \end{tabular}
\end{table}

\section{Data Cleansing}

A massive amount of raw data is piling up in the recent years from different sources and it is been continuously getting stored as the storage mechanism is getting cheaper and the storage capacity increases day by day. This raw data cannot be analysed as it is by human or traditional applications as the processing capacity has exceeded because of the quantity of the data. That is the reason why big data technologies have evolved and they use distributed systems like MapReduce, Spark, Flink etc. Even if we have a big data solution to process the high quantity of raw data, it is not the efficiency and performance of the solution that determines the quality of the knowledge extracted but it depends on the quality of the data as well. The raw data likely be imperfect and containing noise, irrelevant information, missing values etc. It is well known that low quality data will lead to low quality knowledge \cite{preprocessing}. Hence data cleaning is the major step to be performed before we continue with data mining algorithms to make sure that we are using a suitable and relevant data set. 

Data cleaning has 2 parts. First part is data preparation and second part is data reduction techniques. 

\subsubsection{Data Preparation}

The data going to the analytics model should be clean and noise free. Hence data preparation part includes tasks like data cleaning, data normalization, data transformation, missing value imputation, data integration and Noise identification. Figure \ref{fig:Figure4} shows the data preparations tasks \cite{preprocessing}.

\begin{figure}[htb]
  \centering
  \includegraphics[width=1.0\columnwidth]{project/images/Figure4.png}
  \caption{Data Preprocessing or preparation Tasks
  \cite{preprocessing}}
  \label{fig:Figure4} 
\end{figure}

\subsubsection{Data Reduction Techniques}

To reduce the dimensionalty problem and the computational cost, because of the large number of variables and instances in the data set, we try to gather only the required set of quality data. Data reduction techniques include feature selection, instance selection and discretization. Figure \ref{fig:Figure5} shows data reduction techniques \cite{preprocessing}

\begin{figure}[htb]
  \centering
  \includegraphics[width=1.0\columnwidth]{project/images/Figure5.png}
  \caption{Data Reduction Approaches
  \cite{preprocessing}}
  \label{fig:Figure5} 
\end{figure}


\section{Data Analysis}

\subsection{Exploratory Analysis}

Interquartile Range:

The interquartile range (IQR) is a measure of variability, based on dividing a data set into quartiles.

Quartiles divide a rank-ordered data set into four equal parts. The values that divide each part are called the first, second, and third quartiles; and they are denoted by Q1, Q2, and Q3, respectively.

Q1 is the "middle" value in the first half of the rank-ordered data set.
Q2 is the median value in the set.
Q3 is the "middle" value in the second half of the rank-ordered data set.
The interquartile range is equal to Q3 minus Q1.\cite{stat-trek-statistics}

Quartile:

Quartiles divide a rank-ordered data set into four equal parts. The values that divide each part are called the first, second, and third quartiles; and they are denoted by Q1, Q2, and Q3, respectively.

Note the relationship between quartiles and percentiles. Q1 corresponds to P25, Q2 corresponds to P50, Q3 corresponds to P75. Q2 is the median value in the set.

See also:  	AP Statistics Tutorial: Measures of Position



\subsection{Cross-Tabulation}

\section{Data Modelling}

\section{Results}



\section{Conclusion}

Put here an conclusion. Conlcusions and abstracts must not have any
citations in the section.


\begin{acks}

  The authors would like to thank Dr. Gregor von Laszewski for his
  support and suggestions to write this paper.

\end{acks}

\bibliographystyle{ACM-Reference-Format}
\bibliography{report} 


\end{document}
